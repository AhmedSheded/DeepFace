{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Representations for images in  data/dataset/database  folder were previously stored in  representations_vgg_face.pkl . If you added new instances after this file creation, then please delete this file and call find function again. It will create it again.\n",
      "There are  7  representations found in  representations_vgg_face.pkl\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "find function lasts  5.369013786315918  seconds\n"
     ]
    }
   ],
   "source": [
    "df = DeepFace.find(img_path='data/dataset/Omar/10.jpg', db_path='data/dataset/database')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "models = [\"VGG-Face\", \"Facenet\", \"Facenet512\", \"OpenFace\", \"DeepFace\", \"DeepID\", \"ArcFace\", \"Dlib\", \"SFace\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 61s 61s/step\n",
      "1/1 [==============================] - 0s 114ms/step\n"
     ]
    }
   ],
   "source": [
    "result = DeepFace.verify(img1_path='data/dataset/Mahmoud/10.jpg', img2_path='data/dataset/Ahmed/4.jpg', model_name = models[4])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "{'verified': False,\n 'distance': 0.27175460252245354,\n 'threshold': 0.23,\n 'model': 'DeepFace',\n 'detector_backend': 'opencv',\n 'similarity_metric': 'cosine'}"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: age:   0%|          | 0/4 [00:00<?, ?it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 799ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender:  25%|██▌       | 1/4 [00:01<00:04,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 855ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race:  50%|█████     | 2/4 [00:02<00:02,  1.15s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 943ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion:  75%|███████▌  | 3/4 [00:03<00:01,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion: 100%|██████████| 4/4 [00:03<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "obj = DeepFace.analyze(img_path = \"data/d.jpg\", actions = ['age', 'gender', 'race', 'emotion'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "{'age': 25,\n 'region': {'x': 108, 'y': 242, 'w': 315, 'h': 315},\n 'gender': 'Man',\n 'race': {'asian': 0.03847710842968425,\n  'indian': 3.00528369130395,\n  'black': 0.03026306111070878,\n  'white': 14.499009519756408,\n  'middle eastern': 73.98220145596274,\n  'latino hispanic': 8.444770928905662},\n 'dominant_race': 'middle eastern',\n 'emotion': {'angry': 3.779429658123945e-07,\n  'disgust': 8.46241259071595e-21,\n  'fear': 3.064277550777171e-12,\n  'happy': 98.51042628288269,\n  'sad': 9.562420855147025e-08,\n  'surprise': 1.320438798302348e-05,\n  'neutral': 1.489566732198},\n 'dominant_emotion': 'happy'}"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "10"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj['region']['x']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import cv2 as cv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "img = cv.imread('data/d.jpg')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "img = cv.rectangle(img, (108, 242), (108+315, 242+315), (0, 255, 0), 2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "cv.imshow('img', img)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: age:   0%|          | 0/2 [00:00<?, ?it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender:  50%|█████     | 1/2 [00:01<00:01,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 906ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender: 100%|██████████| 2/2 [00:02<00:00,  1.21s/it]\n",
      "Action: age:   0%|          | 0/2 [00:00<?, ?it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 864ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender:  50%|█████     | 1/2 [00:00<00:00,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 709ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]\n",
      "Action: age:   0%|          | 0/2 [00:00<?, ?it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 678ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender:  50%|█████     | 1/2 [00:00<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 714ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender: 100%|██████████| 2/2 [00:01<00:00,  1.28it/s]\n",
      "Action: age:   0%|          | 0/2 [00:00<?, ?it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 690ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender:  50%|█████     | 1/2 [00:00<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 706ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender: 100%|██████████| 2/2 [00:01<00:00,  1.26it/s]\n",
      "Action: age:   0%|          | 0/2 [00:00<?, ?it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender:  50%|█████     | 1/2 [00:01<00:01,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender: 100%|██████████| 2/2 [00:02<00:00,  1.18s/it]\n",
      "Action: age:   0%|          | 0/2 [00:00<?, ?it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 694ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender:  50%|█████     | 1/2 [00:00<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 675ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender: 100%|██████████| 2/2 [00:01<00:00,  1.28it/s]\n",
      "Action: age:   0%|          | 0/2 [00:00<?, ?it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 681ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender:  50%|█████     | 1/2 [00:00<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 704ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender: 100%|██████████| 2/2 [00:01<00:00,  1.27it/s]\n",
      "Action: age:   0%|          | 0/2 [00:00<?, ?it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 672ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender:  50%|█████     | 1/2 [00:00<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 701ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender: 100%|██████████| 2/2 [00:01<00:00,  1.28it/s]\n",
      "Action: age:   0%|          | 0/2 [00:00<?, ?it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 940ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender:  50%|█████     | 1/2 [00:01<00:01,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]\n"
     ]
    }
   ],
   "source": [
    "cap = cv.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        obj = DeepFace.analyze(img_path = frame, actions = ['age', 'gender'])\n",
    "        frame = cv.rectangle(frame, (obj['region']['x'], obj['region']['y']), (obj['region']['x']+obj['region']['w'], obj['region']['y']+obj['region']['h']), (255, 0, 0), 2)\n",
    "        cv.imshow('frame', frame)\n",
    "        if cv.waitKey(30) == 27:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
